{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA/QC for MODULAIR-PM Batch 3.1\n",
    "\n",
    "The goal of this notebook is to determine whether any devices should be fixed or held-back from shipment after a calibration/QA-QC period.\n",
    "\n",
    "## About the QA/QC Process\n",
    "\n",
    "Each batch contains ~50 MODULAIR-PM sensors that are co-located on our rooftop chamber at Greentown Labs. All sensors should be seeing the same air and thus should show the same result. The objective of this notebook is to identify those that do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantaq\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from quantaq.utils import to_dataframe\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import variation\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import probplot\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Variables\n",
    "\n",
    "Here, we define the batch, start, and stop dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = \"Batch 3.1\"\n",
    "\n",
    "start = \"2021-05-30 09:00:00\"\n",
    "end = \"2021-05-31 10:00:00\"\n",
    "resample_length = \"1min\"\n",
    "n_samples = (pd.to_datetime(end) - pd.to_datetime(start))/pd.Timedelta(resample_length)\n",
    "\n",
    "sensor_cols = ['met.pressure', 'met.rh', 'met.temp', 'neph.bin0', 'neph.bin1', 'neph.bin2', 'neph.bin3', 'neph.bin4', 'neph.bin5', 'neph.pm1', 'neph.pm10', 'neph.pm25', 'opc.bin0', 'opc.bin1', 'opc.bin10', 'opc.bin11', 'opc.bin12', 'opc.bin13', 'opc.bin14', 'opc.bin15', 'opc.bin16', 'opc.bin17', 'opc.bin18', 'opc.bin19', 'opc.bin2', 'opc.bin20', 'opc.bin21', 'opc.bin22', 'opc.bin23', 'opc.bin3', 'opc.bin4', 'opc.bin5', 'opc.bin6', 'opc.bin7', 'opc.bin8', 'opc.bin9', 'opc.pm1', 'opc.pm10', 'opc.pm25', 'opc.rh', 'opc.temp']\n",
    "opc_cols = ['opc.bin0', 'opc.bin1', 'opc.bin2', 'opc.bin3', 'opc.bin4', 'opc.bin5', 'opc.bin6', 'opc.bin7', 'opc.bin8', 'opc.bin9']\n",
    "\n",
    "with open('apikey.txt') as f:\n",
    "    api_key = f.read().replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the data\n",
    "\n",
    "Use py-quantaq to pull all data. Each batch of sensors is assigned to a Team and can be referenced as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the API Client\n",
    "client = quantaq.QuantAQAPIClient(api_key=api_key)\n",
    "\n",
    "\n",
    "# Retrieve the devices\n",
    "devices = client.devices.list(team=\"Batch 3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the API to get all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "with tqdm(total=len(devices), desc=\"API Download\") as pbar:\n",
    "    for each in devices:\n",
    "        if (np.datetime64(each['last_seen']) > np.datetime64(start)) & (np.datetime64(each['created']) < np.datetime64(end)):\n",
    "            data = client.data.list(sn=each[\"sn\"], start=start, stop=end, raw=True, per_page=500)\n",
    "            frame = to_dataframe(data)\n",
    "            if len(frame)>0:\n",
    "                nonsensor_cols = ['flag', 'sn', 'timestamp', 'timestamp_local', 'url', 'geo.lat', 'geo.lon']\n",
    "                if frame.drop(nonsensor_cols, axis=1).isna().sum().sum() == 0:\n",
    "                    # Resample?\n",
    "                    frame = frame.resample(resample_length, on='timestamp').mean().reset_index()\n",
    "                    # frame = frame.resample(\"5S\", on='timestamp').mean().reset_index()\n",
    "                    frame[\"sn\"] = each[\"sn\"]\n",
    "                    # Append\n",
    "                    frames.append(frame)\n",
    "                else:\n",
    "                    print(\"WARNING: Unit \" + str(each[\"sn\"]) + \"returning unexpected missing values\")\n",
    "            else:\n",
    "                print(\"WARNING: Unit \" + str(each[\"sn\"]) + \" not recording in timeframe\")\n",
    "        else:\n",
    "            print(\"WARNING: Unit \" + str(each[\"sn\"]) + \" not connecting in timeframe\")\n",
    "        pbar.update(1)\n",
    "    \n",
    "frames = pd.concat(frames)\n",
    "\n",
    "# Set the datatype for the flag\n",
    "frames[\"flag\"] = frames[\"flag\"].astype(\"int8\", errors='ignore')\n",
    "\n",
    "# Drop empty columns\n",
    "frames = frames.dropna(how='all', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick out frames that give a lot of NaNs\n",
    "frames_nnas = frames[[\"sn\",\"opc.bin0\"]].groupby(\"sn\").apply(lambda x: (~x.isna()).sum())[\"opc.bin0\"]\n",
    "highna_sn = list(frames_nnas.index[frames_nnas < (n_samples * 0.85)])\n",
    "if len(highna_sn) > 0:\n",
    "    print(\"WARNING: The following units have a lot of missing data: \" + str(highna_sn))\n",
    "    frames = frames[~frames[\"sn\"].isin(highna_sn)]\n",
    "    \n",
    "frames.set_index(frames[\"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_nnas = frames[[\"sn\",\"opc.bin0\"]].groupby(\"sn\").apply(lambda x: (~x.isna()).sum())[\"opc.bin0\"]\n",
    "(frames_nnas/n_samples).sort_values().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the data\n",
    "\n",
    "Export the data in case we want to re-visit later since the API takes a few minutes depending on length of time downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as feather format\n",
    "frames.reset_index().to_feather(\"{}.{}.{}.feather\".format(\n",
    "    batch.lower().replace(\" \", \".\"), start, end),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data\n",
    "\n",
    "Importing the data if we have already have a batch saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"{}.{}.{}.feather\".format(batch.lower().replace(\" \", \".\"), start, end)\n",
    "frames = pd.read_feather(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Munge the data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=\"neph.bin1\"\n",
    "fig = px.scatter(frames, x=\"timestamp\", y=var, color=\"sn\", title=var+\" Comparison\", render_mode=\"webgl\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollingex.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"neph.bin0\"\n",
    "rollingex = frames.set_index('timestamp', inplace=False)[[\"sn\", var]].groupby(\"sn\", as_index=False).rolling(\"15min\", win_type='gaussian').mean()\n",
    "rollingstd = frames.set_index('timestamp', inplace=False)[[\"sn\", var]].groupby(\"sn\", as_index=False).rolling(\"5min\", win_type='gaussian').std()\n",
    "\n",
    "\n",
    "fig = px.line(rollingex, x=rollingex.index.get_level_values(1), y=var, color=rollingex.index.get_level_values(0), title=var +\" Comparison\", render_mode=\"webgl\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = temp\n",
    "var = \"neph.bin0\"\n",
    "\n",
    "fig = px.line(temp, x=temp.index.get_level_values(1), y=\"neph.bin0\", color=temp.index.get_level_values(0), title=\"neph_steady Comparison\", render_mode=\"webgl\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(neph_norm, x=neph_norm.index.get_level_values(1), y=\"neph.bin0\", color=neph_norm.index.get_level_values(0), title=\"neph_norm Comparison\", render_mode=\"webgl\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a QQ plot to test for normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch data_points definition to switch between looking at the distribution over all time or at a specific time\n",
    "# data_points = frames[frames['timestamp']=='2021-05-08 09:42:00']['opc.bin0'].dropna().sort_values()[:] #we can drop outliers\n",
    "data_points = frames['neph.bin0'].dropna().sort_values()[:]\n",
    "# data_points = pd.Series(np.random.normal(0, 1, size=(500))).sort_values()[:-250]\n",
    "# data_points = pd.Series(np.random.uniform(0, 1, size=(500)))\n",
    "qq = probplot(data_points, dist='norm')\n",
    "x = np.array([qq[0][0][0], qq[0][0][-1]])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(x=qq[0][0], y=qq[0][1], mode='markers')\n",
    "fig.add_scatter(x=x, y=qq[1][1] + qq[1][0]*x, mode='lines')\n",
    "fig.layout.update(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch data_points definition to switch between looking at the distribution over all time or at a specific time\n",
    "# data_points = frames[frames['timestamp']=='2021-05-08 09:42:00']['opc.bin0'].dropna().sort_values()[:] #we can drop outliers\n",
    "data_points = frames['opc.bin0'].dropna().sort_values()[:]\n",
    "# data_points = pd.Series(np.random.normal(0, 1, size=(500))).sort_values()[:-250]\n",
    "# data_points = pd.Series(np.random.uniform(0, 1, size=(500)))\n",
    "qq = probplot(data_points, dist='uniform')\n",
    "x = np.array([qq[0][0][0], qq[0][0][-1]])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(x=qq[0][0], y=qq[0][1], mode='markers')\n",
    "fig.add_scatter(x=x, y=qq[1][1] + qq[1][0]*x, mode='lines')\n",
    "fig.layout.update(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "\n",
    "Here, we make some figures and compute some statistics to better understand which sensors are good to ship and which need more attention. This is an active area of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers by Device\n",
    "\n",
    "Here, we show simple box plots for each variable of interest (`met.rh`, `met.temp`, `opc.bin0`, `neph.bin0`). This approach will indicate sensors that are, on average, outliers relative to the rest of the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=2)\n",
    "\n",
    "l=frames\n",
    "# Humidity\n",
    "wide = l.set_index([\"timestamp\", \"sn\"]).unstack().xs(\"met.rh\", axis=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(\n",
    "        y=wide.mean(),\n",
    "        name=\"met.rh\",\n",
    "        jitter=0.05,\n",
    "        pointpos=-1.5,\n",
    "        marker_size=5,\n",
    "        boxpoints='all',\n",
    "        hovertext=wide.columns\n",
    "    ),\n",
    "    row=1, col=1,\n",
    ")\n",
    "\n",
    "# Temperature\n",
    "wide = l.set_index([\"timestamp\", \"sn\"]).unstack().xs(\"met.temp\", axis=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(\n",
    "        y=wide.mean(),\n",
    "        name=\"met.temp\",\n",
    "        jitter=0.05,\n",
    "        pointpos=-1.5,\n",
    "        marker_size=5,\n",
    "        boxpoints='all',\n",
    "        hovertext=wide.columns\n",
    "    ),\n",
    "    row=1, col=2,\n",
    ")\n",
    "\n",
    "# OPC Bin0\n",
    "wide = l.set_index([\"timestamp\", \"sn\"]).unstack().xs(\"opc.bin0\", axis=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(\n",
    "        y=wide.mean(),\n",
    "        name=\"opc.bin0\",\n",
    "        jitter=0.05,\n",
    "        pointpos=-1.5,\n",
    "        marker_size=5,\n",
    "        boxpoints='all',\n",
    "        hovertext=wide.columns\n",
    "    ),\n",
    "    row=2, col=1,\n",
    ")\n",
    "\n",
    "# Neph Bin0\n",
    "wide = l.set_index([\"timestamp\", \"sn\"]).unstack().xs(\"neph.bin0\", axis=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(\n",
    "        y=wide.mean(),\n",
    "        name=\"neph.bin0\",\n",
    "        jitter=0.05,\n",
    "        pointpos=-1.5,\n",
    "        marker_size=5,\n",
    "        boxpoints='all',\n",
    "        hovertext=wide.columns\n",
    "    ),\n",
    "    row=2, col=2,\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, width=800, title_text=\"{} Comparison\".format(batch))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient of Variation\n",
    "\n",
    "Here, we compute the CV for each of these variables and output as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = []\n",
    "\n",
    "for var in [\"met.temp\", \"met.rh\", \"opc.bin0\", \"neph.bin0\"]:\n",
    "    # Build the frame\n",
    "    wide = frames.set_index([\"timestamp\", \"sn\"]).unstack().xs(var, axis=1)\n",
    "    \n",
    "    # Compute the CV\n",
    "    cv = wide.apply(variation, nan_policy='omit', axis=1)\n",
    "    \n",
    "    # Add the results to a table\n",
    "    cv = cv.describe().to_dict()\n",
    "    cv[\"var\"] = var\n",
    "    \n",
    "    rv.append(cv)\n",
    "    \n",
    "rv = pd.DataFrame(rv)\n",
    "\n",
    "rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "\n",
    "This is an area that needs much more robust analysis from a mathematical perspective. We are most interested in identifying the sensors that are 'noisy', not neccesarily that read high or low. We may want to identify those as well, but it is not as important in the immediate term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 | Mean + StDev\n",
    "\n",
    "The first approach will find the sensors that have the most individual points that are more than 1x and 2x standard deviations from the mean of the population of sensors. Thus, for each point in time, we compute the mean and standard deviation for the entire group of sensors. We then sum the total number of events where this occurs for each sensor. We can then eliminate sensors based on how frequently they fall outside of this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "\n",
    "# list of dfs of lower and upper bound of cutoff\n",
    "m1mins = []\n",
    "m1maxs = []\n",
    "\n",
    "for var in [\"met.temp\", \"met.rh\", \"opc.bin0\", \"neph.bin0\"]:\n",
    "    # Build the frame\n",
    "    wide = frames.set_index([\"timestamp\", \"sn\"]).unstack().xs(var, axis=1)\n",
    "    \n",
    "\n",
    "    # Compute the mean and standard deviation\n",
    "    mean = wide.median(axis=1)\n",
    "    std = wide.std(axis=1)\n",
    "    \n",
    "    m1mins.append((mean - 2*std).to_frame().rename(columns = {0:var}))\n",
    "    m1maxs.append((mean + 2*std).to_frame().rename(columns = {0:var}))\n",
    "    \n",
    "    for c in wide.columns:\n",
    "        noutliers = ((wide[c] < (mean - 2*std)) | (wide[c] > (mean + 2*std))).sum()\n",
    "\n",
    "        # Find the number of non-nan rows\n",
    "        nobs = wide[c].count()\n",
    "\n",
    "        # Compute some stats\n",
    "        rv = {\n",
    "            \"sn\": c,\n",
    "            \"nobs\": nobs,\n",
    "            \"outliers\": noutliers,\n",
    "            \"outliers.pct\": round(100.*noutliers/nobs, 1),\n",
    "            \"var\": var,\n",
    "            \"sn\": c\n",
    "        }\n",
    "\n",
    "        # Add to results\n",
    "        outliers.append(rv)\n",
    "    \n",
    "outliers = pd.DataFrame(outliers)\n",
    "\n",
    "m1min = pd.concat(m1mins, axis=1)\n",
    "m1min[\"timestamp\"] = m1min.index\n",
    "m1max = pd.concat(m1maxs, axis=1)\n",
    "m1max[\"timestamp\"] = m1max.index\n",
    "\n",
    "# Output the Figure\n",
    "fig = px.scatter(outliers, \n",
    "         x=\"outliers.pct\", y=\"var\", color=\"sn\", \n",
    "         log_x=True, height=500, width=700,\n",
    "         title=\"Determining Outliers for {} using a Mean + StDev Approach\".format(batch),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorlist = ['aliceblue', 'antiquewhite', 'aqua', 'aquamarine', 'azure', 'beige', 'bisque', 'black', 'blanchedalmond', 'blue', 'blueviolet', 'brown', 'burlywood', 'cadetblue', 'chartreuse', 'chocolate', 'coral', 'cornflowerblue', 'cornsilk', 'crimson', 'cyan', 'darkblue', 'darkcyan', 'darkgoldenrod', 'darkgray', 'darkgrey', 'darkgreen', 'darkkhaki', 'darkmagenta', 'darkolivegreen', 'darkorange', 'darkorchid', 'darkred', 'darksalmon', 'darkseagreen', 'darkslateblue', 'darkslategray', 'darkslategrey', 'darkturquoise', 'darkviolet', 'deeppink', 'deepskyblue', 'dimgray', 'dimgrey', 'dodgerblue', 'firebrick', 'floralwhite', 'forestgreen', 'fuchsia', 'gainsboro', 'ghostwhite', 'gold', 'goldenrod', 'gray', 'grey', 'green', 'greenyellow', 'honeydew', 'hotpink', 'indianred', 'indigo', 'ivory', 'khaki', 'lavender', 'lavenderblush', 'lawngreen', 'lemonchiffon', 'lightblue', 'lightcoral', 'lightcyan', 'lightgoldenrodyellow', 'lightgray', 'lightgrey', 'lightgreen', 'lightpink', 'lightsalmon', 'lightseagreen', 'lightskyblue', 'lightslategray', 'lightslategrey', 'lightsteelblue', 'lightyellow', 'lime', 'limegreen', 'linen', 'magenta', 'maroon', 'mediumaquamarine', 'mediumblue', 'mediumorchid', 'mediumpurple', 'mediumseagreen', 'mediumslateblue', 'mediumspringgreen', 'mediumturquoise', 'mediumvioletred', 'midnightblue', 'mintcream', 'mistyrose', 'moccasin', 'navajowhite', 'navy', 'oldlace', 'olive', 'olivedrab', 'orange', 'orangered', 'orchid', 'palegoldenrod', 'palegreen', 'paleturquoise', 'palevioletred', 'papayawhip', 'peachpuff', 'peru', 'pink', 'plum', 'powderblue', 'purple', 'red', 'rosybrown', 'royalblue', 'saddlebrown', 'salmon', 'sandybrown', 'seagreen', 'seashell', 'sienna', 'silver', 'skyblue', 'slateblue', 'slategray', 'slategrey', 'snow', 'springgreen', 'steelblue', 'tan', 'teal', 'thistle', 'tomato', 'turquoise', 'violet', 'wheat', 'white', 'whitesmoke', 'yellow', 'yellowgreen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 | 1.5*IQR\n",
    "\n",
    "Here, we do the same thing as above, but instead of using the mean + 2*stdev to determine outliers, we use 1.5 * IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "\n",
    "# list of dfs of lower and upper bound of cutoff\n",
    "m2mins = []\n",
    "m2maxs = []\n",
    "\n",
    "for var in [\"met.temp\", \"met.rh\", \"opc.bin0\", \"neph.bin0\"]:\n",
    "    # Build the frame\n",
    "    wide = frames.set_index([\"timestamp\", \"sn\"]).unstack().xs(var, axis=1)\n",
    "    \n",
    "\n",
    "    # Compute the mean and standard deviation\n",
    "    q25 = wide.quantile(0.25, axis=1)\n",
    "    q75 = wide.quantile(0.75, axis=1)\n",
    "\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    m2mins.append((q25 - 1.5*iqr).to_frame().rename(columns = {0:var}))\n",
    "    m2maxs.append((q75 + 1.5*iqr).to_frame().rename(columns = {0:var}))\n",
    "    \n",
    "    for c in wide.columns:\n",
    "        noutliers = ((wide[c] < q25 - 1.5*iqr) | (wide[c] > q75 + 1.5*iqr)).sum()\n",
    "\n",
    "        # Find the number of non-nan rows\n",
    "        nobs = wide[c].count()\n",
    "\n",
    "        # Compute some stats\n",
    "        rv = {\n",
    "            \"sn\": c,\n",
    "            \"nobs\": nobs,\n",
    "            \"outliers\": noutliers,\n",
    "            \"outliers.pct\": round(100.*noutliers/nobs, 1),\n",
    "            \"var\": var,\n",
    "            \"sn\": c\n",
    "        }\n",
    "\n",
    "        # Add to results\n",
    "        outliers.append(rv)\n",
    "    \n",
    "outliers = pd.DataFrame(outliers)\n",
    "\n",
    "m2min = pd.concat(m2mins, axis=1)\n",
    "m2min[\"timestamp\"] = m2min.index\n",
    "m2max = pd.concat(m2maxs, axis=1)\n",
    "m2max[\"timestamp\"] = m2max.index\n",
    "\n",
    "sns = outliers[outliers[\"var\"]==\"neph.bin0\"].sort_values(by = \"outliers.pct\", ascending = False)[\"sn\"].reset_index(drop = True)\n",
    "\n",
    "# Output the Figure\n",
    "fig = px.scatter(outliers, \n",
    "         x=\"outliers.pct\", y=\"var\", color=\"sn\", \n",
    "         log_x=True, height=500, width=700,\n",
    "         title=\"Determining Outliers for {} using a 1.5*IQR Approach\".format(batch),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "times = m1min['timestamp'].index[::10]\n",
    "\n",
    "# Add traces\n",
    "models = frames[\"sn\"].drop_duplicates()\n",
    "colorsIdx = {i:k for i in models for k in random.choices(colorlist, k=len(models))}\n",
    "cols = frames[frames[\"timestamp\"].isin(times)][\"sn\"].map(colorsIdx)\n",
    "fig.add_trace(go.Scattergl(x=frames[frames[\"timestamp\"].isin(times)][\"timestamp\"],\n",
    "                    y=frames[frames[\"timestamp\"].isin(times)][\"neph.bin0\"],\n",
    "                    text=frames[frames[\"timestamp\"].isin(times)][\"sn\"],\n",
    "                    hoverinfo='text',\n",
    "                    mode='markers',\n",
    "                    name='markers',\n",
    "                    marker=dict(color=cols)))\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=m1min.loc[times]['timestamp'].index,\n",
    "                    y=m1min.loc[times]['neph.bin0'],\n",
    "                    line_color='green',\n",
    "                    mode='lines',\n",
    "                    name='min1'))\n",
    "fig.add_trace(go.Scattergl(x=m1max.loc[times]['timestamp'].index,\n",
    "                    y=m1max.loc[times]['neph.bin0'],\n",
    "                    line_color='green',\n",
    "                    mode='lines',\n",
    "                    name='max1'))\n",
    "fig.add_trace(go.Scattergl(x=m2min.loc[times]['timestamp'].index,\n",
    "                    y=m2min.loc[times]['neph.bin0'],\n",
    "                    line_color='red',\n",
    "                    mode='lines',\n",
    "                    name='min2'))\n",
    "fig.add_trace(go.Scattergl(x=m2max.loc[times]['timestamp'].index,\n",
    "                    y=m2max.loc[times]['neph.bin0'],\n",
    "                    line_color='red',\n",
    "                    mode='lines',\n",
    "                    name='max2'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 | Neighbors\n",
    "Here we look for how far a point is away from it's nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(x):\n",
    "    xd = x.drop(['timestamp','sn'], axis = 1)\n",
    "    # if there are no neighbors, return 0\n",
    "    if len(xd.dropna()) < 2:\n",
    "        xd[\"dist\"] = np.nan\n",
    "    else:\n",
    "        nbrs = NearestNeighbors(n_neighbors=2, # each point is counted as it's own neighbor, so n=2 is nearest 1 neighbor\n",
    "                                algorithm='auto', \n",
    "                                metric='euclidean'\n",
    "                               ).fit(np.array(xd.dropna()).reshape(-1,1))\n",
    "        dists = []\n",
    "        for p in np.array(xd).flatten():\n",
    "            if np.isnan(p):\n",
    "                dists.append(np.nan)\n",
    "            else:\n",
    "                distances, indices = nbrs.kneighbors(np.array([[p]]))\n",
    "                dists.append(np.sum(distances))\n",
    "        # save distances with \"timestamp\" and \"sn\" to be merged back into the frames df later\n",
    "        xd = x[[\"timestamp\",\"sn\"]]\n",
    "        xd[\"dist\"] = dists\n",
    "    return xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge distances back into the origional df \n",
    "frames_dist = frames[['timestamp', 'sn', 'neph.bin0']].merge(frames[['timestamp','neph.bin0','sn']].groupby('timestamp').apply(nn),\n",
    "                                                   how='left',\n",
    "                                                   on=['timestamp','sn']\n",
    "                                                   ).sort_values(by = [\"timestamp\",\"sn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take powers of distances\n",
    "frames_dist['d2'] =  frames_dist['dist']**2\n",
    "frames_dist['d3'] =  frames_dist['dist']**3\n",
    "frames_dist['d4'] =  frames_dist['dist']**4\n",
    "# average within each device\n",
    "sn_df = frames_dist[['sn','dist']].groupby('sn').mean().merge(frames_dist[['sn','d2']].groupby('sn').mean()**(1/2), on='sn')\n",
    "sn_df = sn_df.merge(df[['sn','d3']].groupby('sn').mean()**(1/3), on='sn')\n",
    "sn_df = sn_df.merge(df[['sn','d4']].groupby('sn').mean()**(1/4), on='sn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered list of devices with most suspect first\n",
    "sns = pd.Series(sn_df.sort_values(by=['d2'], ascending=False).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4 | Difference\n",
    "Here we look at the square difference in succesive values, and average over each device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list = ['met.pressure', 'met.rh', 'met.temp', 'neph.bin0', 'neph.bin1', 'neph.bin2', 'neph.bin3', 'neph.bin4', 'neph.bin5', 'neph.pm1', 'neph.pm10', 'neph.pm25', 'opc.bin0', 'opc.bin1', 'opc.bin10', 'opc.bin11', 'opc.bin12', 'opc.bin13', 'opc.bin14', 'opc.bin15', 'opc.bin16', 'opc.bin17', 'opc.bin18', 'opc.bin19', 'opc.bin2', 'opc.bin20', 'opc.bin21', 'opc.bin22', 'opc.bin23', 'opc.bin3', 'opc.bin4', 'opc.bin5', 'opc.bin6', 'opc.bin7', 'opc.bin8', 'opc.bin9', 'opc.pm1', 'opc.pm10', 'opc.pm25', 'opc.rh', 'opc.temp']\n",
    "\n",
    "frames = frames.sort_values(by = [\"timestamp\"])\n",
    "# within each device for each sensor measurement find the difference from the previous measurement and square\n",
    "for col in sensor_list:\n",
    "    frames[col+\".d\"] = frames.groupby(\"sn\")[col].diff()\n",
    "    frames[col+\".d\"] = frames[col+\".d\"]**2\n",
    "\n",
    "#resort\n",
    "frames.sort_values(by = [\"sn\",\"timestamp\"])\n",
    "\n",
    "# average within each device and measurement\n",
    "sn_df = frames[[\"sn\"] + list(map(lambda x: x + \".d\", sensor_list))].groupby(\"sn\").mean()\n",
    "# ordered list of devices with most suspect first\n",
    "sns = pd.Series(sn_df.sort_values(by=[\"neph.bin0.d\"], ascending=False).index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool\n",
    "A graphical tool to identify the problem sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = go.Figure()\n",
    "#sns = frames[\"sn\"].drop_duplicates().reset_index(drop=True) # this will give you and unordered sn list\n",
    "var = \"neph.bin0\"\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scattergl(\n",
    "        visible=True,\n",
    "        mode='markers',\n",
    "        marker=dict(opacity=0.2),\n",
    "        name=\"fleet\",       \n",
    "        x=frames[\"timestamp\"],\n",
    "        y=frames[var]))\n",
    "\n",
    "# Add traces, one for each slider step\n",
    "for step in range(len(sns)):\n",
    "    fig.add_trace(\n",
    "        go.Scattergl(\n",
    "            visible=False,\n",
    "            name=str(sns[step]),\n",
    "            mode='markers',\n",
    "            x=frames[frames[\"sn\"] == sns[step]][\"timestamp\"],\n",
    "            y=frames[frames[\"sn\"] == sns[step]][var]))\n",
    "#            x=frames_list[step][\"timestamp\"],\n",
    "#            y=frames_list[step][var]))\n",
    "\n",
    "# Make 10th trace visible\n",
    "fig.data[1].visible = True\n",
    "\n",
    "# Create and add slider\n",
    "steps = []\n",
    "for i in range(len(fig.data)-1):\n",
    "    showing = [True] + [False] * (len(fig.data)-1)\n",
    "    showing[i+1] = True\n",
    "    step = dict(\n",
    "        label=sns[i],\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": showing},\n",
    "              {\"title\": \"Unit: \" + str(sns[i])}],  # layout attribute\n",
    "    )\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=0,\n",
    "#    type=\"buttons\",\n",
    "    font={\"size\":10},\n",
    "    buttons=steps\n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=sliders\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_full = frames[['opc.bin0', 'opc.bin1', 'opc.bin2', 'opc.bin3', 'opc.bin4', 'opc.bin5', 'opc.bin6', 'opc.bin7', 'opc.bin8', 'opc.bin9', 'sn', \"timestamp\"]].copy()\n",
    "opc_full = opc_full.set_index([\"sn\", \"timestamp\"])\n",
    "sns_corrections = opc_full.groupby(level=0).mean().sum(axis=1)\n",
    "sns_corrections = sns_corrections.mean()/sns_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_full_reg = opc_full.apply(lambda x: x.mul(sns_corrections, level=0))\n",
    "opc_sns_reg = opc_full_reg.groupby(level=\"sn\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(opc_full.loc(slice('2021-05-08 14:30:00','2021-05-08 15:30:00'), :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_sub = opc_full.loc[(slice(None),slice('2021-05-08 14:30:00','2021-05-08 15:30:00')),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime('2021-05-08 14:30:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime('2021-05-08 14:30:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(opc_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = opc_sub\n",
    "fig = px.scatter(df, x=\"opc.bin0\", y=\"opc.bin1\", color=df.index.get_level_values(0), title=\"Neph Bin0 Comparison\", render_mode=\"webgl\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[100, 180],\n",
    "        y=[50, 90],\n",
    "        mode=\"lines\",\n",
    "        line=go.scatter.Line(color=\"gray\"),\n",
    "        showlegend=False)\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[100, 160],\n",
    "        y=[56.25, 90],\n",
    "        mode=\"lines\",\n",
    "        line=go.scatter.Line(color=\"gray\"),\n",
    "        showlegend=False)\n",
    ")\n",
    "\n",
    "fig.show(height=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(opc_sns_reg, x=\"opc.bin1\", y=\"opc.bin2\", color=opc_sns_reg.index.get_level_values(0), title=\"Neph Bin0 Comparison\", render_mode=\"webgl\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_sns_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id=\"graph\", figure=fig),\n",
    "])\n",
    "\n",
    "app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opc_full = frames[['opc.bin0', 'opc.bin1', 'opc.bin2', 'opc.bin3', 'opc.bin4', 'opc.bin5', 'opc.bin6', 'opc.bin7', 'opc.bin8', 'opc.bin9', 'sn']].copy()\n",
    "opc_sns = opc_full.groupby(\"sn\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(opc_sns, x=\"opc.pm1\", y=\"opc.pm10\", color=\"opc.pm25\", title=\"OPC Sorting\", render_mode=\"webgl\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_full[\"correction\"] = opc_full[\"sn\"].map(dict(opc_sns[\"opc.bin0\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_full[\"correction\"] = opc_full[\"sn\"].map(dict(opc_sns[\"opc.bin0\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['opc.bin0', 'opc.bin1', 'opc.bin2', 'opc.bin3', 'opc.bin4', 'opc.bin5', 'opc.bin6', 'opc.bin7', 'opc.bin8', 'opc.bin9']:\n",
    "    # opc_full[col + \".n1\"] = opc_full[col]/opc_full[\"correction\"]\n",
    "    opc_sns[col + \".n1\"] = opc_sns[col]/opc_sns.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_sns.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_sns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_sns[['opc.bin1.n1', 'opc.bin2.n1', 'opc.bin3.n1', 'opc.bin4.n1', 'opc.bin5.n1', 'opc.bin6.n1', 'opc.bin7.n1', 'opc.bin8.n1', 'opc.bin9.n1']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_sns[\"sn\"] = opc_sns.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_sns_long = opc_sns.melt(id_vars=[\"sn\"], var_name = 'bin', value_vars=['opc.bin1.n1', 'opc.bin2.n1', 'opc.bin3.n1', 'opc.bin4.n1', 'opc.bin5.n1', 'opc.bin6.n1', 'opc.bin7.n1', 'opc.bin8.n1', 'opc.bin9.n1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renameing = {'opc.bin1.n1':1, 'opc.bin2.n1':2, 'opc.bin3.n1':3, 'opc.bin4.n1':4, 'opc.bin5.n1':5, 'opc.bin6.n1':6, 'opc.bin7.n1':7, 'opc.bin8.n1':8, 'opc.bin9.n1':9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opc_sns_long[\"bin\"] = opc_sns_long[\"bin\"].map(renameing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(opc_sns_long, x=\"bin\", y=\"value\", color=\"sn\", line_group=\"sn\", title=\"OPC Sorting\", render_mode=\"webgl\", log_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([[1,3,3],[2,4,2],[3,5,1]]).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
