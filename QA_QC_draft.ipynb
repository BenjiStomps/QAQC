{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a17c3d-49be-4784-bb24-514a4d29d816",
   "metadata": {},
   "source": [
    "# Code Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b286b-2863-44d1-8790-5dd3caf3379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantaq\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from quantaq.utils import to_dataframe\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import variation\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import probplot\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from pandas.errors import MergeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907a7c8-ae04-4b2d-9a77-940a67954bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch, start, end, resample_length=\"1min\", na_limit=0.15):\n",
    "    \"\"\"\n",
    "    `load_data` will download data from the testing chamber, \n",
    "    report and drop units with issues connecting, \n",
    "    and return a DataFrame. \n",
    "\n",
    "    :param batch: The name of the batch to pass to \n",
    "        the API for downloading.\n",
    "    :type batch: string\n",
    "    :param start: The start time of the data of interest, \n",
    "        formatted like: \"2021-05-30 09:00:00\".\n",
    "    :type start: string\n",
    "    :param end: The end time of the data of interest, \n",
    "        formatted like: \"2021-05-30 09:00:00\".\n",
    "    :type end: string\n",
    "    :param resample_length: The interval to be used in \n",
    "        resampling, defaults to \"1min\".\n",
    "    :type resample_length: string\n",
    "    :param na_limit: The maximum portion of reports that a unit \n",
    "        can skip (it is assumed that each unit should report as \n",
    "            many times as the most-reporting unit), defaults to 0.15.\n",
    "    :type na_limit: float\n",
    "    :return: A DataFrame with all the data, indexed by \"timestamp\".\n",
    "    :rtype: DataFrame\n",
    "    \"\"\"\n",
    "    sensor_cols = ['met.pressure', 'met.rh', 'met.temp', 'neph.bin0', \n",
    "                   'neph.bin1', 'neph.bin2', 'neph.bin3', 'neph.bin4', \n",
    "                   'neph.bin5', 'neph.pm1', 'neph.pm10', 'neph.pm25', \n",
    "                   'opc.bin0', 'opc.bin1', 'opc.bin10', 'opc.bin11', \n",
    "                   'opc.bin12', 'opc.bin13', 'opc.bin14', 'opc.bin15', \n",
    "                   'opc.bin16', 'opc.bin17', 'opc.bin18', 'opc.bin19', \n",
    "                   'opc.bin2', 'opc.bin20', 'opc.bin21', 'opc.bin22', \n",
    "                   'opc.bin23', 'opc.bin3', 'opc.bin4', 'opc.bin5', \n",
    "                   'opc.bin6', 'opc.bin7', 'opc.bin8', 'opc.bin9', \n",
    "                   'opc.pm1', 'opc.pm10', 'opc.pm25', 'opc.rh', 'opc.temp']\n",
    "    nonsensor_cols = ['flag', 'sn', 'timestamp', 'timestamp_local', \n",
    "                      'url', 'geo.lat', 'geo.lon']   \n",
    "    \n",
    "    # Setup the API Client\n",
    "    client = quantaq.QuantAQAPIClient()\n",
    "    \n",
    "    # Retrieve the devices\n",
    "    devices = client.devices.list(team=\"Batch 3.1\")\n",
    "    \n",
    "    frames = []\n",
    "    unit_len = {}\n",
    "\n",
    "    with tqdm(total=len(devices), desc=\"API Download\") as pbar:\n",
    "        for each in devices:\n",
    "            if ((np.datetime64(each['last_seen']) > np.datetime64(start)) \n",
    "                & (np.datetime64(each['created']) < np.datetime64(end))):\n",
    "                data = client.data.list(sn=each[\"sn\"], start=start, \n",
    "                                        stop=end, raw=True, per_page=500)\n",
    "                frame = to_dataframe(data)\n",
    "                unit_len[each[\"sn\"]] = len(frame)\n",
    "                if len(frame)>0:\n",
    "                    if (frame.drop(nonsensor_cols, axis=1).isna().sum().sum() \n",
    "                        == 0):\n",
    "                        # Resample?\n",
    "                        frame = (frame\n",
    "                                 .resample(resample_length, on='timestamp')\n",
    "                                 .mean()\n",
    "                                 .reset_index()\n",
    "                                )\n",
    "                        frame[\"sn\"] = each[\"sn\"]\n",
    "                        # Append\n",
    "                        frames.append(frame)\n",
    "                    else:\n",
    "                        na_frame = frame.drop(nonsensor_cols, axis=1).isna()\n",
    "                        print(f\"WARNING: Unit {each['sn']} returning \"\n",
    "                              f\"unexpected missing values for \"\n",
    "                              f\"{list(na_frame.columns[na_frame.sum()>0])}\")\n",
    "                else:\n",
    "                    print(f\"WARNING: Unit {each['sn']} not recording \"\n",
    "                          \"in timeframe\")\n",
    "            else:\n",
    "                print(f\"WARNING: Unit {each['sn']} not connecting \"\n",
    "                      \"in timeframe\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    frames = pd.concat(frames)\n",
    "\n",
    "    # Set the datatype for the flag\n",
    "    frames[\"flag\"] = frames[\"flag\"].astype(\"int8\", errors='ignore')\n",
    "\n",
    "    # Drop empty columns\n",
    "    frames = frames.dropna(how='all', axis=1)\n",
    "    \n",
    "    # Pick out frames that give a lot of NaNs\n",
    "    for key in unit_len:\n",
    "        unit_len[key] = 100*unit_len[key]/max(unit_len.values())\n",
    "    low_data_sns = {key:val for (key,val) in unit_len.items() if val<85}\n",
    "    if len(low_data_sns) > 0:\n",
    "        for sn,missing_data_pct in low_data_sns.items():\n",
    "            print(f\"WARNING: Unit {sn} only reports \"\n",
    "                  f\"{str(missing_data_pct)[:4]}% as much data as other units\")\n",
    "        frames = frames[~frames[\"sn\"].isin(list(low_data_sns.keys()))]\n",
    "    \n",
    "    # all non- or low-reporting units have been filtered out at the point\n",
    "    sns_list = list(frames[\"sn\"].unique())\n",
    "    print(f\"The number of recording units is {len(sns_list)}\")\n",
    "    \n",
    "    # Set timestamp as index\n",
    "    frames.set_index(frames[\"timestamp\"], inplace=True)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "def find_window(df, var):\n",
    "    \"\"\"\n",
    "    `find_window` creates a line plot of each of the quartiles.\n",
    "\n",
    "    :param df: The data from which to compute the quantiles.\n",
    "    :type df: DataFrame\n",
    "    :param var: The variable to plot. Must be a column in the DataFrame.\n",
    "    :type var: string\n",
    "    :return: A line plot.\n",
    "    :rtype: plotly graphics object\n",
    "    \"\"\"\n",
    "    \n",
    "    var_trend = (df[[var]]\n",
    "                 .groupby(\"timestamp\")\n",
    "                 .quantile(q=[0.25, 0.5, 0.75])\n",
    "                 .reset_index(level=1)\n",
    "                 .rename(columns={\"level_1\":\"quantile\"})\n",
    "                )\n",
    "    fig = px.line(var_trend, x=var_trend.index, y=var, color=\"quantile\", \n",
    "                  title= f\"Median {var} over time\", render_mode=\"webgl\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea433978-c840-4f78-ab22-b4c3d79d095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_units(df, var, steady_start=None, steady_end=None, threshold_factor=2):\n",
    "    \"\"\"\n",
    "    `flag_units` does something...\n",
    "\n",
    "    :param df: The data to check for anomalies.\n",
    "    :type df: DataFrame\n",
    "    :param var: The variable to check for anomalies.\n",
    "    :type var: string\n",
    "    :param steady_start: Start of the time window to check for \n",
    "        anomalies. \n",
    "    :type steady_start: string, optional\n",
    "    :param steady_end: End of the time window to check for anomalies.\n",
    "    :type steady_end: string, optional\n",
    "    :param threshold_factor: A unit will be flagged when it's \"error\" \n",
    "        is more than `threashold_factor` times as large as the \n",
    "        median \"error\".\n",
    "    :type threshold_factor: float\n",
    "    :return: a list of the prolbem units; a record of the \n",
    "        standard deviations for each unit\n",
    "    :rtype: list; DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # build a multiindexed df with var as column, sn and timestamp as \n",
    "    # index, and restricted to the time between steady_start and steady_end\n",
    "    df_steady = df[[var,\"sn\", \"timestamp\"]].copy()\n",
    "    df_steady[\"timestamp\"] = pd.to_datetime(df_steady[\"timestamp\"])\n",
    "    df_steady.set_index([\"sn\",\"timestamp\"], inplace=True)\n",
    "    if not (steady_start==None or steady_end==None):\n",
    "        df_steady = (\n",
    "         df_steady.loc[(slice(None),slice(steady_start,steady_end)),:])\n",
    "    \n",
    "    # compute the scaling factors (neph_scale) needed so that each module \n",
    "    # will have the same mean over time (as in neph_scaled)\n",
    "    var_scale = df_steady.groupby(level=0).mean()\n",
    "    var_scale = var_scale.median()/var_scale\n",
    "    var_scaled = df_steady*var_scale\n",
    "    \n",
    "    var_scaled_trend = var_scaled.groupby(level=1).median()\n",
    "    var_std_list = (var_scaled\n",
    "                    .divide(var_scaled_trend, axis=0)\n",
    "                    .groupby(level=0)\n",
    "                    .std()\n",
    "                   )\n",
    "    var_std_threshold = var_std_list.median()[0]*threshold_factor\n",
    "    \n",
    "    problem_units = list(\n",
    "        (var_std_list[var_std_list[var] > var_std_threshold])\n",
    "        .index\n",
    "    )\n",
    "    \n",
    "    if len(problem_units)>0:\n",
    "        print(f\"Median standard deviation is {var_std_list.median()[0]}\")\n",
    "        print(f\"Modules which significantly deviate from the median in {var}:\")\n",
    "        print(\n",
    "            var_std_list\n",
    "            .loc[problem_units]\n",
    "            .sort_values(by=var, ascending=False)\n",
    "        )\n",
    "        \n",
    "        return problem_units, var_std_list\n",
    "    else:\n",
    "        print(f\"No issues found with {var}.\")\n",
    "        return [], var_std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7fdaf-c148-487d-aa9b-8faaa6b60673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scales(df, problem_dict, steady_start=None, steady_end=None):\n",
    "    \"\"\"\n",
    "    In a given time window `scales` computes a scaling factor for \n",
    "    each unit so that each unit will have the same mean value over \n",
    "    the time window as the median unit does.\n",
    "\n",
    "    :param df: The data to be scaled.\n",
    "    :type df: DataFrame\n",
    "    :param problem_dict: A dictionary which returns the problem \n",
    "        units for a given variable.\n",
    "    :type problem_dict: dictionary\n",
    "    :param steady_start: Start of the time window to scale by. \n",
    "    :type steady_start: string, optional\n",
    "    :param steady_end: End of the time window to scale by.\n",
    "    :type steady_end: string, optional\n",
    "    :return: The data unscaled; the data scaled; a dictionary \n",
    "        which gives the scalling factors for a given variable.\n",
    "    :rtype: DataFrame; DataFrame; dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    df= df.copy()\n",
    "    \n",
    "    df_scaled = pd.DataFrame()\n",
    "    scale_dict = {}\n",
    "    \n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df.set_index([\"sn\", \"timestamp\"], inplace=True)\n",
    "    df_unscaled = df[list(problem_dict.keys())].copy()\n",
    "    for var in problem_dict.keys():\n",
    "        if not (steady_start==None or steady_end==None):\n",
    "            df_steady = (\n",
    "                df.loc[(slice(None),slice(steady_start,steady_end)),var].copy()\n",
    "            )\n",
    "        else:\n",
    "            df_steady = df.loc[:,var].copy()\n",
    "        # compute the scaling factors (var_scale) needed so that each \n",
    "        # module will have the same mean over time (as in neph_scaled)\n",
    "        var_scale = df_steady.groupby(level=0).mean()\n",
    "        var_scale = var_scale.drop(problem_dict[var]).median()/var_scale\n",
    "        scale_dict[var] = var_scale\n",
    "        df_scaled.loc[:,var] = df_unscaled.loc[:,var]*var_scale\n",
    "    \n",
    "    return df_unscaled, df_scaled, scale_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9b6c3-3ef4-42c8-8dc6-21aad6f894cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quartiles(df, problem_dict):\n",
    "    \"\"\"\n",
    "    `quartiles` builds a DataFrame which gives the quartiles for each \n",
    "    of the variables\n",
    "\n",
    "    :param df: The data to be processed.\n",
    "    :type df: DataFrame\n",
    "    :param problem_dict: A dictionary which for a given variable \n",
    "        will return the problem units from that variable to be \n",
    "            excluded from the quartile analysis.\n",
    "    :type problem_dict: dictionary\n",
    "    :return: A DataFrame indexed by quartile, with variables as columns.\n",
    "    :rtype: DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    df_quartile = pd.DataFrame()\n",
    "    for var in list(problem_dict.keys()):\n",
    "        df_quartile[var] =(\n",
    "            df\n",
    "            .drop(problem_dict[var], level=\"sn\")\n",
    "            .groupby(level=1)\n",
    "            .quantile(q=[0.25,0.50,0.75])\n",
    "            .rename_axis([\"timestamp\",\"Q\"])[var]\n",
    "        )\n",
    "    return df_quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2d059-b6cc-4583-84a8-127f6b2f0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plot(df, df_q, units, var):\n",
    "    \"\"\"\n",
    "    `find_window` does something...\n",
    "\n",
    "    :param df: \n",
    "    :type df: \n",
    "    :param df_q: \n",
    "    :type df_q: \n",
    "    :param units: list of units to plot a line for\n",
    "    :type units: list of strings\n",
    "    :param var: variable to plot\n",
    "    :type var: string\n",
    "    :return: ???\n",
    "    :rtype: ???\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scattergl(\n",
    "        x=list(\n",
    "            df_q\n",
    "            .loc[(slice(None),0.50),var]\n",
    "            .index\n",
    "            .get_level_values(\"timestamp\")\n",
    "        ),\n",
    "        y=list(df_q.loc[(slice(None),0.50),var]),\n",
    "        hoverinfo=\"skip\",\n",
    "        line=dict(color='rgba(0,0,0,1)'),\n",
    "        name=\"median\"\n",
    "    ))\n",
    "    for unit in units:\n",
    "        fig.add_trace(go.Scattergl(\n",
    "            x = (df\n",
    "                 .loc[(unit,slice(None)),:]\n",
    "                 .index\n",
    "                 .get_level_values(\"timestamp\")),\n",
    "            y = df.loc[(unit,slice(None)),:][var],\n",
    "            mode = 'lines',\n",
    "            name = unit\n",
    "        ))\n",
    "    fig.add_trace(go.Scattergl(\n",
    "        x=(list(df_q\n",
    "                .loc[(slice(None),0.25),var]\n",
    "                .index\n",
    "                .get_level_values(\"timestamp\")\n",
    "               )\n",
    "           + list(df_q\n",
    "                  .loc[(slice(None),0.75),var]\n",
    "                  .index\n",
    "                  .get_level_values(\"timestamp\")\n",
    "                 )[::-1]\n",
    "          ),\n",
    "        y=(list(df_q.loc[(slice(None),0.25),var])\n",
    "           + list(df_q.loc[(slice(None),0.75),var])[::-1]),\n",
    "        fill='toself',\n",
    "        fillcolor='rgba(20,20,20,0.2)',\n",
    "        hoverinfo=\"skip\",\n",
    "        line=dict(color='rgba(190,190,190,0)'),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b2f4e-cf8a-43f6-96a1-cea74272e352",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc8403-f3e4-45e6-a171-47e3a3faaf24",
   "metadata": {},
   "source": [
    "TO DO: make it easy to set different time windows for different variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d209875c-720d-478a-89cc-6afbfccd2a45",
   "metadata": {},
   "source": [
    "Below we load all the data into `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d3ee1-ffb6-4ccf-b648-7b33f57ff25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(batch=\"Batch 3.1\", \n",
    "               start=\"2021-06-16 01:00:00\", \n",
    "               end=\"2021-06-17 12:00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbd0c8-739d-4fc0-9830-3ff941444e7a",
   "metadata": {},
   "source": [
    "Look at the below graph of humidity to see if any units appear not to be in the chamber.  If so you can make a list `not_in_chamber` and remove them from `df` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33072ee-ead5-4275-bc9e-7c363f068c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x=\"timestamp\", y=\"met.rh\", color=\"sn\", render_mode=\"webgl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2e27e-9c84-4cb0-ab75-85bd3182e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_chamber=[\"MOD-PM-00230\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15f52e-cc1c-44b3-a84a-f9fbbb59b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[\"sn\"].isin(not_in_chamber)]\n",
    "all_units = df[\"sn\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f3c924-a2db-4a94-8ef6-867fd618c12d",
   "metadata": {},
   "source": [
    "Now that we know all the units in `df` are in the chamber.  We can use the below plot, to define an appropriate steady state window below.  Rerun for different variables as necessary. Probably better to go on the smaller side to pick something appropriate for all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c4edc7-1d6f-4d51-94a2-f1c53e09504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_window(df,\"neph.bin0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d949a-d972-4458-ab39-c2ff5dfb604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "steady_start = \"2021-06-16 01:00:00\"\n",
    "steady_end = \"2021-06-16 22:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bb46f-5709-4176-b129-01b865e780ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_dict = {} \n",
    "var_std_list = pd.DataFrame()\n",
    "for var in [\"neph.bin0\", \"opc.bin0\", \"opc.bin1\", \"opc.bin2\", \"opc.bin3\"]:\n",
    "    out1, out2 = flag_units(df, \n",
    "                            var, \n",
    "                            steady_start=steady_start, \n",
    "                            steady_end=steady_end, \n",
    "                            threshold_factor=1.5\n",
    "                           )\n",
    "    if var_std_list.empty:\n",
    "        var_std_list = out2\n",
    "    else:\n",
    "        var_std_list = var_std_list.merge(out2, \n",
    "                                          left_index=True, \n",
    "                                          right_index=True\n",
    "                                         )\n",
    "    problem_dict[var] = out1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30ab292-2a31-4e25-ad96-193c0d78270d",
   "metadata": {},
   "source": [
    "Now we will pick out the five best units.  We pick the best by summing the cost over all variables.  We might consider doing a weighted sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca8d96-71c1-4987-ad72-d88723ea93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_units = list(var_std_list.sum(axis=1).sort_values()[:5].index)\n",
    "print(best_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd72656-eeff-4e85-8e61-04f2f29d845c",
   "metadata": {},
   "source": [
    "We will build a DataFrame, `df_scaled`, with the values for each unit scaled so that the average for each unit in a given variable is the same as the median unit for that variable.  The DataFrame is multiindexed by `sn` and `timestamp`, just like `df_unscaled` which is the same without the scaling factor havingn been applied. Given a variable, `scale_dict` will return the scaling factor for each unit for that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1c101-a72a-4960-8c93-17504cfba515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unscaled, df_scaled, scale_dict = scales(df, \n",
    "                                            problem_dict, \n",
    "                                            steady_start=steady_start, \n",
    "                                            steady_end=steady_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893781bd-fcc8-4056-8319-c686432f1693",
   "metadata": {},
   "source": [
    "For each variable (scaled and also unscaled) we will compute the median and the quartiles at each timestep so we can and trend lines to our charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d87436e-6eb4-4eb5-9e7f-3054d38d3ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quartile_scaled = quartiles(df_scaled, problem_dict)\n",
    "df_quartile_unscaled = quartiles(df_unscaled, problem_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928aef1-146a-4475-b3b4-bdeb33f30889",
   "metadata": {},
   "source": [
    "Now we may build some graphs! Be sure to use the (un)scaled quartiles with the (un)scaled data.  And if you're using `problem_dict` be sure the key matches the variable you're plotting.  Here are a couple examples were we look at scaled data for `opc.bin1` for\n",
    "* all units\n",
    "* all units except for the ones that were flagged\n",
    "* the flagged units\n",
    "* the five best units (these are the best averaged  across all variables, not just `opc.bin1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f7ea8-95b3-40da-a703-97a742b19138",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_plot(df_scaled, df_quartile_scaled, all_units, \"opc.bin1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ad64b-ca3e-423e-8bbc-4fc393d806ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_plot(df_scaled, \n",
    "           df_quartile_scaled, \n",
    "           list(set(all_units)-set(problem_dict[\"neph.bin1\"])), \n",
    "           \"neph.bin1\"\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934a2f8-289d-4836-a642-854a5aeb0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_plot(df_scaled, \n",
    "           df_quartile_scaled, \n",
    "           problem_dict[\"opc.bin1\"], \n",
    "           \"opc.bin1\"\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ebd532-6026-4ed6-ae99-00b5c62d0b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_plot(df_scaled, df_quartile_scaled, best_units, \"opc.bin1\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
